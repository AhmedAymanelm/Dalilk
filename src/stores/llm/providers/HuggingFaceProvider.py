from ..LLMinterfacefactory import LLMInterfaceFactory
from ..llmEnum import HuggingFaceENUM
import logging
from typing import List
import torch
import os


os.environ.setdefault("TRANSFORMERS_NO_TF", "1")


class HuggingFaceProvider(LLMInterfaceFactory):
    def __init__(
        self,
        api_key: str = None,
        api_url: str = None,
        defult_input_max_character: int = 1000,
        defult_output_max_character: int = 1000,
        defult_generation_temperature: float = 0.1,
    ):
        self.api_key = api_key
        self.api_url = api_url
        self.defult_input_max_character = defult_input_max_character
        self.defult_output_max_character = defult_output_max_character
        self.defult_generation_temperature = defult_generation_temperature

        self.generate_model_id = None
        self.emmbedding_model_id = None
        self.embedding_size = None
        self.embedding_model = None  # SentenceTransformer model

        self.enums = HuggingFaceENUM
        self.logger = logging.getLogger(__name__)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø² (GPU Ø£Ùˆ CPU)
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.logger.info(f"ğŸ”§ Using device: {self.device}")


    def set_generation_model(self, model_id: str):
        """
        Ø­Ø§Ù„ÙŠØ§Ù‹ Hugging Face Ù„Ù„Ù€ generation ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯
        ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Groq Ø£Ùˆ OpenAI Ù„Ù„Ù€ generation
        """
        self.generate_model_id = model_id
        self.logger.warning("âš ï¸ Hugging Face generation not implemented. Use Groq/OpenAI for generation.")

    def set_Emmbidding_model(self, model_id: str, embedding_size: int):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings Ù…Ù† Hugging Face"""
        self.emmbedding_model_id = model_id
        self.embedding_size = embedding_size
        
        try:
            # Lazy import Ù„ØªØ¬Ù†Ø¨ ØªØ­Ù…ÙŠÙ„ TensorFlow
            from sentence_transformers import SentenceTransformer
            
            self.logger.info(f"ğŸ“¥ Loading Hugging Face model: {model_id}")
            self.embedding_model = SentenceTransformer(model_id, device=self.device)
            self.logger.info(f"âœ… Model loaded successfully on {self.device}")
        except Exception as e:
            self.logger.error(f"âŒ Failed to load model {model_id}: {e}")
            raise

    def set_embedding_model(self, model_id: str, embedding_size: int):
        return self.set_Emmbidding_model(model_id, embedding_size)

    def process_text(self, text: str):
        return text[: self.defult_input_max_character].strip()

    def generate_text(self, prompt: str, chat_history: list = [], max_out_tokens: int = None, temperature: float = None):
        """
        âš ï¸ Generation ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…
        Ø§Ø³ØªØ®Ø¯Ù… Groq Ø£Ùˆ OpenAI Ù„Ù„Ù€ generation
        """
        self.logger.error("âŒ Hugging Face generation not implemented")
        raise NotImplementedError("Use Groq or OpenAI for text generation")

    def embed_text(self, text: str, dcoument_type: str = None):
        """
        ØªØ­ÙˆÙŠÙ„ Ù†Øµ ÙˆØ§Ø­Ø¯ Ø¥Ù„Ù‰ embedding Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Hugging Face
        """
        if not self.embedding_model:
            raise ValueError("âŒ Embedding model not loaded. Call set_embedding_model() first.")
        
        try:
            processed_text = self.process_text(text)
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ embedding
            embedding = self.embedding_model.encode(
                processed_text,
                convert_to_tensor=False,  # Ù†Ø±Ø¬Ø¹ numpy array
                show_progress_bar=False
            )
            return embedding.tolist()
        except Exception as e:
            self.logger.error(f"âŒ Error generating embedding: {e}")
            raise

    def embed(self, texts: List[str], model: str = None, input_type: str = None):
        """
        ØªØ­ÙˆÙŠÙ„ Ø¹Ø¯Ø© Ù†ØµÙˆØµ Ø¥Ù„Ù‰ embeddings Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© (Ø£Ø³Ø±Ø¹)
        """
        if not self.embedding_model:
            raise ValueError("âŒ Embedding model not loaded. Call set_embedding_model() first.")
        
        try:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
            processed_texts = [self.process_text(text) for text in texts]
            
            # ØªØ­ÙˆÙŠÙ„ Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© (Ø£Ø³Ø±Ø¹ Ù…Ù† ÙˆØ§Ø­Ø¯ ÙˆØ§Ø­Ø¯)
            embeddings = self.embedding_model.encode(
                processed_texts,
                convert_to_tensor=False,
                show_progress_bar=False,
                batch_size=32  # ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù€ batch
            )
            
            return [emb.tolist() for emb in embeddings]
        except Exception as e:
            self.logger.error(f"âŒ Error generating embeddings: {e}")
            raise

    def constract_prompt(self, prompt: str, role: str):
        return {"role": role, "content": self.process_text(prompt)}
